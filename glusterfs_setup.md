###Начальная настройка
#####Конфигурация машин
В этом отчете будет использоваться 3 машины: 2 сервера и 1 клиент
1-й сервер **glusterfs-srv1** (IP 192.168.100.20)
2-й сервер **glusterfs-srv2** (IP 192.168.100.21)
клиент **glusterfs-client1** (IP 192.168.100.22)
#####Предварительная настройка
Устанавливаем wget и attr
```
# yum install wget attr
```
в файле `/etc/hosts` прописываем все машины
```
192.168.100.20 glusterfs-srv1
192.168.100.21 glusterfs-srv2
192.168.100.22 glusterfs-client1
```
Добавляем репозиторий GlusterFS
```
# cd /etc/yum.repos.d/
# wget https://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/glusterfs-epel.repo
```
На **сервере** устанавливаем
```
# yum install glusterfs-server
```
На **клиенте** устанавливаем
```
# yum install glusterfs-client
```
На серверах открываем TCP порты
**111** - портмаппер
**24007** - демон GlusterFS
**24008** - порт для менеджмента

**24009** и далее (GlusterFs < **v3.4**)
или
**49152** и далее (GlusterFs < **v3.4**) 

Каждый новый брик(любая директория, которая присоединенна к пулу) задействует новый порт. Например, если у нас используется 3 брика, нужно открыть порты 24009-24011 или 49152-4914, в зависимости от версии GlusterFS

#####Форматирование и монтирование дисков на сервере, конфигурация автозагрузки
Подключаем жесткие диски, форматируем и монтируем их (в этом примере только раздел /dev/sdb1)
```
# mkfs.ext4 /dev/sdb1
# mount /dev/sdb1 /mnt/data
```
Для автоматического монтирование при старте системы прописываем в `/etc/fstab`
```
/dev/sdb1 /mnt/data ext4 defaults 1 2
```
Запускаем GlusterFS, добавляем его в автозагрузку
```
# systemctl start glusterd
# systemctl enable glusterd
```
#####Объединение серверов в кластер
После того, как все сконфигурировали, на первом сервере выполняем
```
# gluster peer probe glusterfs-srv2
```
На втором (опционально)
```
# gluster peer probe glusterfs-srv1
```
После успешного выполнения `gluster peer probe ` должны получать вывод `peer probe: success`, иначе нужно проверить порты и всю остальную конфигурацию.

Если все прошло успешно, перезапускаем службы `glusterd` на всех серверах
```
# systemctl restart glusterd
```
И проверяем, все ли работает так, как надо
```
gluster pool list
```
В выводе должена быть таблица `UUID`, `Hostname` и `Status`. В нашем случае там будет две строчки (два сервера в пуле) и статусы будут `Connected`

